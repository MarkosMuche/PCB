{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "import main\n",
    "from gym.spaces import Discrete, Box\n",
    "import random\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "class PsecWorld(Env):\n",
    "    def __init__(self, state, W, H):\n",
    "        self.H= H\n",
    "        self.W = W\n",
    "        self.action_space = Discrete(len(W))\n",
    "        self.observation_space= np.array(range(1,len(W)+2))\n",
    "        self.state = state\n",
    "        self.stop = 100\n",
    "        \n",
    "    def reset(self,state):\n",
    "        self.state = state\n",
    "        self.stop = 100\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        action -= 1\n",
    "        self.action = action\n",
    "        self.stop -= 1\n",
    "        r, s_ = main.move(self.state,self.W, self.H, action)\n",
    "    \n",
    "        self.state = s_\n",
    "        if r == 0:\n",
    "            reward = 0\n",
    "        if r > 0:\n",
    "            reward = 1\n",
    "        if r< 0:\n",
    "            reward = -1\n",
    "        if self.stop <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseq=[[1,2,4,6,3,5],[3,2,6,1,4,5]]\n",
    "w=[1,2,3,2,1,1]\n",
    "h=[2,1,4,3,3,2]\n",
    "action=3\n",
    "state = pseq[0]\n",
    "state.append(action)\n",
    "\n",
    "env = PsecWorld(state, w, h)\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape\n",
    "y = Box(low=np.array([[0, 0]]), high=np.array([[4,4]]))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.action_space.n\n",
    "print(state)\n",
    "state = np.reshape(state,[-1,env.observation_space.shape[0]])\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.reset(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "class Memory:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.batch_s = []\n",
    "        self.batch_a = []\n",
    "        self.batch_r = []\n",
    "        self.batch_gae_r = [] #this gets set in agent make_gae which is called once on first training on memory\n",
    "        self.batch_s_ = []\n",
    "        self.batch_done = []\n",
    "        self.GAE_CALCULATED_Q = False #make sure make_gae can only be called once\n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size):\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            s,a,r,gae_r,s_,d = [],[],[],[],[],[]\n",
    "            pos = np.random.randint(len(self.batch_s)) #random position\n",
    "            s.append(self.batch_s[pos])\n",
    "            a.append(self.batch_a[pos])\n",
    "            r.append(self.batch_r[pos])\n",
    "            gae_r.append(self.batch_gae_r[pos])\n",
    "            s_.append(self.batch_s_[pos])\n",
    "            d.append(self.batch_done[pos])\n",
    "        return s,a,r,gae_r,s_,d #return randomized batches\n",
    "\n",
    "\n",
    "    def store(self, s, a, s_, r, done):\n",
    "\n",
    "        self.batch_s.append(s)\n",
    "        self.batch_a.append(a)\n",
    "        self.batch_r.append(r)\n",
    "        self.batch_s_.append(s_)\n",
    "        self.batch_done.append(done)\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "\n",
    "        self.batch_s.clear()\n",
    "        self.batch_a.clear()\n",
    "        self.batch_r.clear()\n",
    "        self.batch_s_.clear()\n",
    "        self.batch_done.clear()\n",
    "        self.GAE_CALCULATED_Q = False\n",
    "\n",
    "\n",
    "    def cnt_samples(self):\n",
    "        return len(self.batch_s)\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,action_n, state_dim, training_batch_size):\n",
    "        \n",
    "        self.action_n = action_n\n",
    "        self.state_dim = state_dim        \n",
    "        #CONSTANTS\n",
    "        self.TRAINING_BATCH_SIZE = training_batch_size\n",
    "        self.TARGET_UPDATE_ALPHA = 0.95\n",
    "        self.GAMMA = 0.99\n",
    "        self.GAE_LAMBDA = 0.95\n",
    "        self.CLIPPING_LOSS_RATIO = 0.1\n",
    "        self.ENTROPY_LOSS_RATIO = 0.001\n",
    "        self.TARGET_UPDATE_ALPHA = 0.9\n",
    "        #create actor and critic neural networks\n",
    "        self.critic_network = self._build_critic_network()\n",
    "        self.actor_network = self._build_actor_network()\n",
    "        #for the loss function, additionally \"old\" predicitons are required from before the last update.\n",
    "        #therefore create another networtk. Set weights to be identical for now.\n",
    "        self.actor_old_network = self._build_actor_network()\n",
    "        self.actor_old_network.set_weights(self.actor_network.get_weights()) \n",
    "        #for getting an action (predict), the model requires it's ususal input, but advantage and old_prediction is only used for loss(training). So create dummys for prediction only\n",
    "        self.dummy_advantage = np.zeros((1, 1))\n",
    "        self.dummy_old_prediciton = np.zeros((1, self.action_n))\n",
    "        #our transition memory buffer        \n",
    "        self.memory = Memory()\n",
    "        \n",
    "\n",
    "    \n",
    "    def _build_actor_network(self):\n",
    "\n",
    "        #define inputs. Advantage and old_prediction are required to pass to the ppo_loss funktion\n",
    "        state = K.layers.Input(shape=self.state_dim,name='state_input')\n",
    "        advantage = K.layers.Input(shape=(1,),name='advantage_input')\n",
    "        old_prediction = K.layers.Input(shape=(self.action_n,),name='old_prediction_input')\n",
    "        #define hidden layers\n",
    "        dense = K.layers.Dense(32,activation='relu',name='dense1')(state)\n",
    "        dense = K.layers.Dense(32,activation='relu',name='dense2')(dense)\n",
    "        #connect layers, output action using softmax activation\n",
    "        policy = K.layers.Dense(self.action_n, activation=\"softmax\", name=\"actor_output_layer\")(dense)\n",
    "        #make keras.Model\n",
    "        actor_network = K.Model(inputs = [state,advantage,old_prediction], outputs = policy)\n",
    "        #compile. Here the connection to the PPO loss fuction is made. The input placeholders are passed.\n",
    "        actor_network.compile(\n",
    "            optimizer='Adam',\n",
    "            loss = self.ppo_loss(advantage=advantage,old_prediction=old_prediction)\n",
    "            )\n",
    "        #summary and return       \n",
    "        actor_network.summary()\n",
    "        time.sleep(1.0)\n",
    "        return actor_network\n",
    "\n",
    "\n",
    "    def _build_critic_network(self):\n",
    "\n",
    "        #define input layer\n",
    "        state = K.layers.Input(shape=self.state_dim,name='state_input')\n",
    "        #define hidden layers\n",
    "        dense = K.layers.Dense(32,activation='relu',name='dense1')(state)\n",
    "        dense = K.layers.Dense(32,activation='relu',name='dense2')(dense)\n",
    "        #connect the layers to a 1-dim output: scalar value of the state (= Q value or V(s))\n",
    "        V = K.layers.Dense(1, name=\"actor_output_layer\")(dense)\n",
    "        #make keras.Model\n",
    "        critic_network = K.Model(inputs=state, outputs=V)\n",
    "        #compile. Here the connection to the PPO loss fuction is made. The input placeholders are passed.\n",
    "        critic_network.compile(optimizer='Adam',loss = 'mean_squared_error')\n",
    "        #summary and return           \n",
    "        critic_network.summary()\n",
    "        time.sleep(1.0)\n",
    "        return critic_network\n",
    "    \n",
    "\n",
    "    def ppo_loss(self, advantage, old_prediction):\n",
    "\n",
    "        #refer to Keras custom loss function intro to understand why we define a funciton inside a function.\n",
    "        def loss(y_true, y_pred):\n",
    "            prob = y_true * y_pred #y_true is taken action one_hot(in deterministic case) and pred is a softmax vector. prob is the probability of the taken aciton.\n",
    "            old_prob = y_true * old_prediction\n",
    "            ratio = prob / (old_prob + 1e-10)\n",
    "            clip_ratio = K.backend.clip(ratio, min_value=1 - self.CLIPPING_LOSS_RATIO, max_value=1 + self.CLIPPING_LOSS_RATIO)\n",
    "            surrogate1 = ratio * advantage\n",
    "            surrogate2 = clip_ratio * advantage\n",
    "            entropy_loss = (prob * K.backend.log(prob + 1e-10)) #optionally add the entropy loss to avoid getting stuck on local minima\n",
    "            ppo_loss = -K.backend.mean(K.backend.minimum(surrogate1,surrogate2) + self.ENTROPY_LOSS_RATIO * entropy_loss)\n",
    "            return ppo_loss\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def make_gae(self):\n",
    "\n",
    "        gae = 0\n",
    "        mask = 0\n",
    "        for i in reversed(range(self.memory.cnt_samples)):\n",
    "            mask = 0 if self.memory.batch_done[i] else 1\n",
    "            v = self.get_v(self.memory.batch_s[i])\n",
    "            delta = self.memory.batch_r[i] + self.GAMMA * self.get_v(self.memory.batch_s_[i]) * mask - v\n",
    "            gae = delta + self.GAMMA *  self.GAE_LAMBDA * mask * gae\n",
    "            self.memory.batch_gae_r.append(gae+v)\n",
    "        self.memory.batch_gae_r.reverse()\n",
    "        self.memory.GAE_CALCULATED_Q = True\n",
    "\n",
    "\n",
    "    def update_tartget_network(self):\n",
    "\n",
    "        alpha = self.TARGET_UPDATE_ALPHA\n",
    "        actor_weights = np.array(self.actor_network.get_weights())\n",
    "        actor_tartget_weights = np.array(self.actor_old_network.get_weights())\n",
    "        new_weights = alpha*actor_weights + (1-alpha)*actor_tartget_weights\n",
    "        self.actor_old_network.set_weights(new_weights)\n",
    "\n",
    "    \n",
    "    def choose_action(self,state):\n",
    "\n",
    "        assert isinstance(state,np.ndarray)\n",
    "        #reshape for predict_on_batch which requires 2d-arrays\n",
    "        state = np.reshape(state,[-1,self.state_dim[0]])\n",
    "        #the probability list for each action is the output of the actor network given a state\n",
    "        prob = self.actor_network.predict_on_batch([state,self.dummy_advantage, self.dummy_old_prediciton]).flatten()\n",
    "        #action is chosen by random with the weightings accoring to the probability\n",
    "        action = np.random.choice(self.action_n,p=prob)\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def train_network(self):\n",
    "\n",
    "        #important: make gae type rewards BEFORE getting random batches if not done yet\n",
    "        if not self.memory.GAE_CALCULATED_Q:\n",
    "            self.make_gae()\n",
    "        #get randomized mini batches\n",
    "        states,actions,rewards,gae_r,next_states,dones = self.memory.get_batch(self.TRAINING_BATCH_SIZE)\n",
    "       \n",
    "        #create np array batches for training\n",
    "        batch_s = np.vstack(states)\n",
    "        batch_a = np.vstack(actions)\n",
    "        batch_gae_r = np.vstack(gae_r)\n",
    "        #get values of states in batch\n",
    "        batch_v = self.get_v(batch_s)\n",
    "        #calc advantages. required for actor loss. \n",
    "        batch_advantage = batch_gae_r - batch_v\n",
    "        batch_advantage = K.utils.normalize(batch_advantage) #\n",
    "        #calc old_prediction. Required for actor loss.\n",
    "        batch_old_prediction = self.get_old_prediction(batch_s)\n",
    "        #one-hot the actions. Actions will be the target for actor.\n",
    "        batch_a_final = np.zeros(shape=(len(batch_a), self.action_n))\n",
    "        batch_a_final[:, batch_a.flatten()] = 1\n",
    "\n",
    "        #commit training\n",
    "        self.actor_network.fit(x=[batch_s, batch_advantage, batch_old_prediction], y=batch_a_final, verbose=0)\n",
    "        self.critic_network.fit(x=batch_s, y=batch_gae_r, epochs=1, verbose=0)\n",
    "        #soft update the target network(aka actor_old). \n",
    "        self.update_tartget_network()\n",
    "\n",
    "\n",
    "    def store_transition(self, s, a, s_, r, done):\n",
    "\n",
    "        self.memory.store(s, a, s_, r, done)\n",
    "\n",
    "\n",
    "    def get_v(self,state):\n",
    "\n",
    "        s = np.reshape(state,(-1, self.state_dim[0]))\n",
    "        v = self.critic_network.predict_on_batch(s)\n",
    "        return v\n",
    "    \n",
    "\n",
    "    def get_old_prediction(self, state):\n",
    "\n",
    "        state = np.reshape(state, (-1, self.state_dim[0]))\n",
    "        return self.actor_old_network.predict_on_batch([state,self.dummy_advantage, self.dummy_old_prediciton])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ITERATIONS = 1000\n",
    "MAX_EPISODE_LENGTH = 1000\n",
    "TRAJECTORY_BUFFER_SIZE = 32\n",
    "BATCH_SIZE = 16\n",
    "RENDER_EVERY = 100\n",
    "\n",
    "pseq=[[1,2,4,6,3,5],[3,2,6,1,4,5]]\n",
    "w=[1,2,3,2,1,1]\n",
    "h=[2,1,4,3,3,2]\n",
    "\n",
    "state = pseq[0]\n",
    "state.append(1)\n",
    "\n",
    "env = PsecWorld(state, w, h)\n",
    "agent = Agent(env.action_space.n,env.observation_space.shape,BATCH_SIZE)\n",
    "samples_filled = 0\n",
    "\n",
    "for cnt_episode in range(TRAIN_ITERATIONS):\n",
    "    s = env.reset(state)\n",
    "    r_sum = 0\n",
    "    for cnt_step in range(MAX_EPISODE_LENGTH):\n",
    "        #sometimes render\n",
    "        if cnt_episode % RENDER_EVERY == 0 :\n",
    "            env.render()\n",
    "        #get action from agent given state\n",
    "        a = agent.choose_action(s)\n",
    "        #get s_,r,done\n",
    "        s_, r, done, _ = env.step(a)\n",
    "        r /= 100\n",
    "        r_sum += r\n",
    "        if done:\n",
    "            r = -1\n",
    "        #store transitions to agent.memory\n",
    "        agent.store_transition(s, a, s_, r, done)\n",
    "        samples_filled += 1\n",
    "        #train in batches one buffer is filled with samples.\n",
    "        if samples_filled % TRAJECTORY_BUFFER_SIZE == 0 and samples_filled != 0:\n",
    "            #To be sample efficient, sample as often as statistically necearry to \n",
    "            # use all availible samples in memory. Imortant to sample randomly \n",
    "            # to keep the training data independant and identically distributed IID\n",
    "            for _ in range(TRAJECTORY_BUFFER_SIZE // BATCH_SIZE):\n",
    "                agent.train_network()\n",
    "            agent.memory.clear()\n",
    "            samples_filled = 0\n",
    "        #set state to next_state\n",
    "        s = s_\n",
    "        if done:\n",
    "            break\n",
    "    if cnt_episode % 10 == 0:\n",
    "        print(f\"Episode:{cnt_episode}, step:{cnt_step}, r_sum:{r_sum}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
